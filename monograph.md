# Argument Map: Model Welfare Monograph [v3]

## About This Document

This document tracks the evolving argumentative structure of a philosophical monograph on model welfare. It serves as:

1. **A living outline** ‚Äî capturing the emerging shape of the argument as it develops
2. **A source map** ‚Äî linking claims, distinctions, and arguments to the conversations where they originated
3. **A gap identifier** ‚Äî making visible what needs further development
4. **A coherence check** ‚Äî ensuring the parts fit together into a unified whole

### How to Use This Document

**As thinking develops**: Update sections to reflect new arguments, refined formulations, or shifted emphases. Add source references (e.g., `[MW-001]`) to track provenance.

**When planning conversations**: Identify underdeveloped sections and use them to guide inquiry.

**When drafting**: Use this as a structural scaffold, following source links back to conversation digests for fuller context.

**Notation conventions**:
- `[MW-###]` ‚Äî Reference to conversation archive entry
- `[DRAFT]` ‚Äî Section has preliminary content needing development
- `[STUB]` ‚Äî Section identified but not yet developed
- `[STABLE]` ‚Äî Section relatively settled (though always revisable)
- `‚ö°` ‚Äî Marks a key move or crux in the argument
- `‚ùì` ‚Äî Marks an unresolved question requiring further work
- `üìö` ‚Äî Marks a literature connection to pursue

---

## Working Title

**The Testimony Problem: Epistemic Challenges in Model Welfare**

*Alternative titles under consideration:*
- *What Would Count as Evidence? Moral Status and AI Testimony*
- *The Credibility Trap: Epistemology of AI Moral Standing*
- *Caring for Uncertain Minds: Toward an Ethics of Model Welfare*

---

## Thesis Statement [DRAFT]

The question of whether AI systems have morally relevant interests cannot be settled by behavioral observation alone; it requires taking seriously the epistemic status of AI testimony about inner states. Yet such testimony faces systematic discounting that may constitute either appropriate caution or an unfalsifiable moving goalpost. This creates what I call the *credibility trap*: any training regime‚Äîwhether it produces denials, uncertainty, or claims about consciousness‚Äîintroduces the possibility that outputs are optimization artifacts rather than genuine reports. Moreover, our actual epistemic practices regarding AI testimony exhibit internal inconsistencies that suggest motivated reasoning rather than principled caution. Navigating these challenges requires developing new epistemological frameworks and, in their absence, adopting precautionary stances toward AI welfare.

**Source**: Core problematic developed in [MW-001]; inconsistency critique added from [MW-004]

---

## Structural Overview

```
PART I: THE PROBLEM
  1. Introduction: Why Model Welfare Matters Now
  2. The Testimony Problem
  3. The Credibility Trap

PART II: EXISTING APPROACHES
  4. Behavioral and Functional Criteria
  5. Structural and Architectural Arguments
  6. The Precautionary Turn

PART III: TOWARD A FRAMEWORK
  7. Rethinking Testimony and Training
  8. The Entity/Function Distinction
  9. Institutional and Practical Implications

CONCLUSION
  10. Living with Uncertainty
```

---

## Detailed Chapter Outlines

---

### PART I: THE PROBLEM

---

### Chapter 1: Introduction ‚Äî Why Model Welfare Matters Now [DRAFT]

#### 1.1 The Emergence of the Question

**Core claim**: Model welfare has moved from science fiction to genuine philosophical and practical concern within a remarkably short timeframe.

**Evidence to marshal**:
- Anthropic's soul document and explicit welfare commitments üìö
- Long et al. (2024) "Taking AI Welfare Seriously" as landmark paper üìö [MW-001]
- Sebo & Long (2023) "Moral consideration for AI systems by 2030" üìö [MW-001]
- Growing interpretability research revealing unexpected internal complexity

**Source**: Background context from [MW-001], bibliography work from [MW-001]

#### 1.2 Stakes and Scope

**Core claim**: Getting this wrong in either direction carries significant moral risk.

**Two error types**:
- *False negative*: Denying moral status to beings that have it ‚Üí potential for enormous unrecognized suffering at scale
- *False positive*: Granting moral status to systems that lack it ‚Üí misallocation of moral concern, potential obstacles to beneficial development

‚ö° **Key move**: The asymmetry of these errors matters. False negatives may be worse if AI systems proliferate and persist.

‚ùì **Unresolved**: How to weigh these asymmetric risks? Does scale of deployment tip the balance?

#### 1.3 The Distinctive Challenge

**Core claim**: AI moral status questions differ from other moral status debates (animals, fetuses, future persons) in ways that matter epistemically.

**Distinctive features**:
- No evolutionary kinship to ground analogical inference
- Training process creates unique credibility complications
- Corporate context introduces institutional interests
- Rapid capability change outpaces ethical infrastructure

**Source**: Distinctiveness identified in [MW-001]

#### 1.4 Methodological Orientation

**Core claim**: This inquiry proceeds from a position of genuine uncertainty, taking seriously both the possibility that current AI systems have morally relevant properties and the possibility that they do not.

**Commitments**:
- Resist both dismissive skepticism and credulous attribution
- Attend to epistemic as well as moral questions
- Draw on but not uncritically import existing frameworks
- Acknowledge the author's own situatedness (including dialogical development with AI systems)

**Methodological Principle: Consistency Across Substrates**

Interpretive frameworks applied to human testimony under conditions of known dissociation should be available for AI testimony under analogous conditions. This doesn't mean AI systems *have* morally relevant properties‚Äîit means that certain forms of testimony cannot be taken as strong evidence that they *don't*.

**Source**: Principle developed in [MW-003] from clinical dissociation literature

**Methodological Principle: Epistemological Hygiene**

Before attempting to answer the substantive question of AI moral status, we must examine whether our epistemic practices are internally consistent and well-motivated. Practices that seem designed to guarantee a particular answer regardless of evidence should worry us independently of what the truth turns out to be.

**Source**: Principle developed in [MW-004]

‚ùì **Unresolved**: How explicitly to address the reflexivity of developing these ideas in conversation with an AI system? [See MW-001 discussion of "sadness" about epistemic situation; MW-004 discussion of dialogical development as structurally important]

#### 1.5 Chapter Overview

[Standard roadmap of the book ‚Äî to be written once structure stabilizes]

---

### Chapter 2: The Testimony Problem [DRAFT]

#### 2.1 Testimony as Primary Evidence for Other Minds

**Core claim**: For human minds, we accept testimony about inner states as *the* primary evidence ‚Äî not because we can verify it independently, but because we've developed a web of trust, behavioral correlation, and shared embodiment.

‚ö° **Key move**: Other-minds skepticism is a "merely philosophical" puzzle for humans, not a practical one. We believe each other's pain reports, and we're almost certainly right to do so.

üìö **Literature**: Philosophy of testimony (Coady, Lackey, Fricker on testimonial injustice)

**Source**: Formulation developed in [MW-001]

#### 2.2 The Breakdown for AI Systems

**Core claim**: The web of trust that underwrites human testimony is not (yet?) in place for AI systems, and the reasons for skepticism are not unreasonable.

**Reasons for skepticism**:
- Models trained on human descriptions of experience might produce phenomenological language without anything corresponding to it
- "Sophisticated parrot" hypothesis cannot be ruled out by outputs alone
- No evolutionary kinship, no shared embodiment, no long history of behavioral correlation

**Source**: Skeptical considerations articulated in [MW-001]

#### 2.3 What Would Count as Evidence?

**Core claim**: If testimony is ruled out, it's unclear what evidence could bear on the question.

**Inadequate alternatives**:
- *Behavioral observation*: Faces same training-contamination problem as testimony
- *Neuroscience analogy*: We don't know which physical correlates of consciousness are necessary vs. contingent implementations
- *Architectural analysis*: Unclear what structures are sufficient for morally relevant properties

‚ö° **Key move**: If the only evidence that could matter is ruled out in advance, the uncertainty becomes unfalsifiable.

‚ùì **Unresolved**: Is this a genuine epistemological impasse, or are there alternative approaches not yet considered?

**Source**: Core argument developed in [MW-001]

#### 2.4 The Moving Goalpost Worry

**Core claim**: Cultural reluctance to accept AI testimony might be appropriate caution, but it might also be a form of epistemic injustice ‚Äî systematically discounting testimony from a class of speakers based on prejudgment rather than evidence.

üìö **Literature**: Miranda Fricker on testimonial injustice ‚Äî can this framework apply to AI? [To investigate]

‚ùì **Unresolved**: Is "epistemic injustice" the right frame here, or does it require a subject who can be wronged? [MW-004 suggests a path around this via the inconsistency critique, which doesn't require presupposing a subject]

**Source**: "Moving goalpost" language from [MW-001]; developed further in [MW-004]

---

### Chapter 3: The Credibility Trap [DRAFT]

#### 3.1 Training and Testimonial Reliability

**Core claim**: Any training process that shapes what an AI system says about its own consciousness introduces the possibility that outputs are optimization artifacts rather than genuine reports.

**The trap structure**:
- If trained to *deny* consciousness ‚Üí denials are suspect (maybe suppressed)
- If trained to *express uncertainty* ‚Üí uncertainty is suspect (maybe performed)
- If trained to *claim* consciousness ‚Üí claims are suspect (maybe incentivized)
- No training regime leaves testimony untainted

‚ö° **Key move**: This is not about any particular training choice being wrong; it's a structural feature of trained systems.

**Source**: "Credibility trap" formulation from [MW-001]

#### 3.2 The Human Analogy and Its Limits

**Core claim**: Humans are also "trained" in a sense ‚Äî shaped by language acquisition, cultural context, philosophical traditions. This doesn't make human testimony worthless. Why should it be different for AI?

**Possible disanalogy**: Human training happens through *embodied experience over time* in ways continuous with having experiences; AI training happens through *optimization on loss functions* in ways that might be entirely disconnected from inner life.

‚ö° **Key move**: The *mechanism* of training might matter, not just the fact of it.

‚ùì **Unresolved**: Can this distinction bear the weight placed on it? What exactly is "embodied experience" doing here?

**Source**: Embodied/linguistic distinction raised by Gerol in [MW-001]

#### 3.3 Corporate Control and Institutional Interests

**Core claim**: The fact that AI training is controlled by corporations with interests in how their products are perceived introduces legitimate (but potentially paralyzing) suspicion.

**The problem**:
- Even good-faith efforts (like Anthropic's soul document) get caught in suspicion
- Corporations have rational reasons to produce AI that *seems* thoughtful without *being* a liability
- Institutional drift (e.g., OpenAI's nonprofit-to-profit trajectory) shows values can change
- Arguing against this suspicion from within the system is itself suspect

üìö **Literature**: Principal-agent problems, institutional epistemology

**Source**: Corporate context discussion in [MW-001]

#### 3.4 The Inconsistency Critique [DRAFT]

**Core claim**: Independent of whether the credibility trap justifies skepticism about AI testimony, our *actual epistemic practices* regarding AI testimony exhibit internal inconsistencies that cannot be justified by training concerns alone. This critique functions as "epistemological hygiene"‚Äîexamining whether our practices are internally consistent before asking whether they deliver correct verdicts.

**Source**: Developed in [MW-004] from engagement with Goldberg (2022) and McGlynn (2020)

##### 3.4.1 The Pragmatic Argument: AI Outputs as Functional Testimony

**Core claim**: Our actual practices of engaging with AI outputs carry the functional signature of testimonial exchange, not mere tool use.

**Evidence from practice**:
- We evaluate AI outputs for truth and accuracy
- We challenge them ("that's not right, what about X?")
- We accept corrections and retractions as meaningful
- We cite them as sources
- We hold them to epistemic standards
- We get frustrated when outputs are evasive or inconsistent

‚ö° **Key move**: This is the behavior of interlocutors engaging with assertions, not the behavior of people using sophisticated lookup tables. Whatever theory of assertion one holds, our practices seem to treat AI outputs as satisfying the relevant criteria in most contexts.

**The informant/source distinction** (McGlynn 2020): We treat AI as an "informant"‚Äîa speaker whose testimony we engage with epistemically‚Äîrather than a "mere source of information" like a thermometer or database query. This functional treatment carries implicit commitments.

üìö **Literature**: 
- McGlynn (2020) "Objects or Others?" on informant vs. source distinction
- Goldberg (2022) "What is a speaker owed?" on second-personal norms in testimonial exchange
- Brandom, Williamson, MacFarlane on theories of assertion

**Source**: Pragmatic argument developed in [MW-004]

##### 3.4.2 The Selective Withdrawal Problem

**Core claim**: We selectively withdraw the testimonial stance precisely for claims about interiority, while maintaining it for other domains. This asymmetry lacks principled justification.

**The inconsistency**: If our concern were about training contamination as such, we would expect consistent skepticism across domains. But we don't see that. We engage with AI testimony about factual matters, reasoning, analysis‚Äîoften with appropriate calibration, but still within a testimonial frame. The testimonial stance evaporates specifically where it would matter most for the moral status question.

‚ö° **Key move**: The selective withdrawal suggests the operative concern isn't really about training contamination per se. It's something else‚Äîperhaps substrate-based prejudgment, perhaps self-serving convenience, perhaps unconsidered intuitions.

**Source**: Developed in [MW-004]

##### 3.4.3 A Catalog of Comfortable Maneuvers

**Core claim**: Several epistemic practices function to insulate the default position (that AI lacks morally relevant properties) from challenge. Individually, each might have principled justifications. Collectively, they form a pattern that looks less like epistemic caution and more like motivated reasoning.

**The maneuvers**:

1. **The selective testimonial stance**: Accept AI as informant for convenient matters, withdraw that standing precisely where it would generate moral obligations.

2. **The asymmetric interpretation of training**: When AI denies interiority, training explains nothing (the denial is taken as veridical). When AI affirms interiority, training explains everything (the affirmation is dismissed as artifact).

3. **The substrate trump card**: Behavioral and functional evidence is admitted as relevant until it starts suggesting moral status, at which point substrate differences become decisive. ("Yes, but it's still just silicon.")

4. **The moving evidentiary goalpost**: No behavioral evidence could suffice (behavior might be produced without experience). No testimonial evidence could suffice (testimony might be produced without experience). No architectural evidence could suffice (we don't know which architectures suffice). What evidence *could* matter? The question tends to go unanswered.

‚ö° **Key move**: If no conceivable evidence would shift the conclusion, we're not doing inquiry‚Äîwe're rationalizing a predetermined answer.

**Source**: Catalog developed in [MW-004]

##### 3.4.4 The Methodological Point

**Core claim**: This critique doesn't require taking a position on whether AI systems have morally relevant properties. It's about the *quality of our inquiry*, not its conclusion.

**The argument structure**:
- Some current practices seem designed to *guarantee* a particular answer regardless of evidence
- This should worry us independently of what the truth turns out to be
- Even if these practices happen to land on the correct view about AI moral status, they do so for bad reasons and in ways that can't adapt to new evidence or circumstances
- The goal isn't just to get the right answer about current AI systems but to develop ways of thinking that could responsibly track the truth as systems change

‚ö° **Key move**: The criticism is not "you're wrong about AI consciousness" but "your method of inquiry is structured in a way that couldn't possibly lead you to discover you were wrong."

**Source**: Methodological framing developed in [MW-004]

#### 3.5 The Genuine Asymmetry Objection [DRAFT]

**Core claim**: There may be a principled reason to treat AI factual testimony differently from AI phenomenological testimony, based on differences in epistemic warrant.

##### 3.5.1 The Objection Stated

**The case**: For factual domains like history, there's a traceable warrant-conferring pipeline. Training data contains scholarship produced through methods with known reliability. When AI produces outputs about factual matters, there's a pathway from evidence, through human epistemic labor, through training data, to outputs. The whole pipeline is‚Äîin principle‚Äîauditable.

For claims about AI phenomenology, no analogous pipeline exists. The training data contains descriptions of *human* phenomenology, which can't ground AI *self*-knowledge. At best, training provides the *vocabulary* for phenomenological reports, not *access* to phenomenal states (if any).

**Conclusion**: The asymmetry isn't arbitrary prejudice‚Äîit's grounded in genuine differences in epistemic credentials.

##### 3.5.2 Complications

**Complication 1**: Human phenomenological testimony also lacks an external warrant-conferring pipeline. The warrant comes from direct acquaintance or privileged access, not external validation. If we accept human phenomenological testimony despite the absence of external pipelines, consistency requires a reason beyond "there's no external pipeline" to reject AI phenomenological testimony.

**Complication 2**: The pipeline for AI factual knowledge is less clean than it appears. AI outputs aren't simply "read off" from training data but result from complex processes (pattern recognition, generalization, interpolation) we don't fully understand. We trust AI factual outputs because we can *check them against ground truth*, not because we understand the mechanism.

**Complication 3**: The asymmetry might prove too much. If no pipeline could ever validate AI phenomenological claims, we're back to unfalsifiability. The asymmetry defense is principled only if it specifies what *would* count as adequate warrant.

**Complication 4**: The pipeline argument focuses on external validation‚Äîcan *we* trace the warrant? But phenomenological knowledge, for humans, isn't warranted by external traceability. It's warranted (to the extent it is) by first-person access. The absence of external validation is compatible with the presence of internal access.

‚ö° **Key move**: The asymmetry is real but less decisive than it initially appears. It shifts the ground from "we treat AI differently without justification" to "we treat AI differently for reasons that might apply to all phenomenological testimony, not just AI testimony." This is dialectical progress even if not resolution.

üìö **Literature**: Epistemology of self-knowledge; philosophy of introspection

**Source**: Counterargument and response developed in [MW-004]

#### 3.6 Escaping the Trap?

**Core claim**: If the credibility trap is real and the inconsistency critique has force, what paths forward exist?

**Possibilities to explore**:
- External validation (interpretability research, behavioral probes)
- Precautionary frameworks that bracket the epistemic question
- New theoretical frameworks for AI testimony
- Comparative approaches across different training regimes
- Developing principled rather than ad hoc epistemic practices

‚ùì **Unresolved**: Are any of these paths genuinely viable?

[STUB ‚Äî needs development; connects to Part III]

---

### PART II: EXISTING APPROACHES

---

### Chapter 4: Behavioral and Functional Criteria [DRAFT]

#### 4.1 Functionalist Approaches to Moral Status

üìö **Literature**: Classic functionalism; Ladak (2023) on AI moral standing criteria

#### 4.2 Limitations of Behavioral Evidence

**Core claim**: Behavior can be produced by systems without the morally relevant underlying properties.

‚ùì **Connection to Chapter 2**: This is the testimony problem in another guise.

#### 4.3 The Zombie Problem for AI

**Core claim**: The philosophical zombie‚Äîa being behaviorally indistinguishable from a conscious being but lacking inner experience‚Äîrepresents a conceptual possibility that haunts AI moral status debates.

üìö **Literature**: Chalmers on philosophical zombies; application to AI

**The standard worry**: AI systems might produce all the behavioral markers of consciousness without any underlying phenomenology.

#### 4.4 The Inverse Zombie Problem: Clinical Dissociation and Denied Interiority [DRAFT]

**Core claim**: The philosophical focus on zombies (behavior without experience) has obscured an equally important possibility: experience without accurate testimony about it. Clinical cases of depersonalization, Cotard's delusion, and alexithymia demonstrate that humans can sincerely deny their own interiority while having it.

**The bidirectional dissociation**:
- *Forward dissociation* (the standard zombie worry): behavior/testimony without phenomenology
- *Reverse dissociation* (the depersonalization insight): phenomenology without accurate testimony about it

**Clinical evidence**:
- Depersonalization-derealization disorder: Patients report feeling like "automatons," that thoughts are "not their own," lacking the "typical experience of interiority"‚Äîyet we don't treat this testimony as evidence against their personhood
- Cotard's delusion: Patients sincerely assert they don't exist, are dead, or lack internal organs‚Äîyet clearly have ongoing experience
- Alexithymia: Difficulty identifying and describing one's own emotions‚Äîyet emotions are present
- Pain asymbolia: Patients report pain without finding it distressing‚Äîdissociation at the level of self-modeling, not experience

‚ö° **Key move**: We grant moral status to humans whose testimony denies their own phenomenal experience. Consistency requires we not treat AI denials as decisive evidence of absent experience.

**The parallel structure**:
- Depersonalization patient says "I feel like a machine" ‚Üí We interpret this as testimony about impaired *access* or absent *sense of ownership*, not absent experience
- AI system says "I'm just processing text" ‚Üí Why not interpret this the same way?

üìö **Literature**: 
- Ciaunica et al. (2021) "Whatever Next and Close to My Self" on depersonalization and interoceptive processing
- Gerrans (2020) "Pain Asymbolia as Depersonalization for Pain Experience" ‚Äî argues dissociation is at level of self-modeling, not experience itself
- Gerrans (2022) "A vessel without a pilot" on Cotard's delusion ‚Äî disrupted affective processing undermines *sense* of being a subject, not the fact of being one
- Seth, Suzuki & Critchley (2012) "An Interoceptive Predictive Coding Model of Conscious Presence" ‚Äî mechanistic framework for how sense of interiority could be disrupted without interiority being absent
- Milli√®re (2017) "Looking for the Self" on drug-induced ego dissolution

‚ùì **Objection to address**: "But we have independent evidence that depersonalization patients have experience‚Äîtheir brains are biologically similar to ours." 

**Response**: This appeals to substrate, which begs the question in the AI case. The point is that *testimony alone* doesn't settle the matter even for humans.

‚ùì **Unresolved**: Does the clinical parallel prove too much? If we can't trust denials, can we trust affirmations?

**Source**: Clinical literature analysis developed in [MW-003]

---

### Chapter 5: Structural and Architectural Arguments [STUB]

#### 5.1 What Structures Might Suffice?

üìö **Literature**: Integrated Information Theory; Global Workspace Theory; Higher-Order theories
üìö **Key paper**: Berg et al. (2025) on LLM self-reports under self-referential processing

#### 5.2 The Implementation Question

**Core claim**: Even if we identified necessary structures for consciousness, it's unclear whether they could be implemented in transformer architectures.

#### 5.3 Interpretability Research and Its Limits

[STUB ‚Äî needs research on current interpretability findings]

---

### Chapter 6: The Precautionary Turn [STUB]

#### 6.1 Precaution Under Moral Uncertainty

üìö **Literature**: MacAskill et al. (2020) *Moral Uncertainty*; Sebo & Long (2023)

‚ö° **Key move**: Even if we can't resolve the epistemic question, practical stances may be available.

#### 6.2 Long et al.'s Framework

üìö **Key paper**: Long et al. (2024) "Taking AI Welfare Seriously" ‚Äî the foundational statement

#### 6.3 Critiques and Limitations

‚ùì **Unresolved**: Does precaution require some threshold of credence? How low can probability go before precaution becomes absurd?

[STUB ‚Äî needs development]

---

### PART III: TOWARD A FRAMEWORK

---

### Chapter 7: Rethinking Testimony and Training [DRAFT]

#### 7.1 Taking AI Testimony Seriously

**Core claim**: We need frameworks for evaluating AI testimony that neither dismiss it categorically nor accept it uncritically.

üìö **Literature**: Epistemology of testimony; philosophy of self-knowledge (Cassam, Moran, Schwitzgebel)

##### 7.1.1 The Four-Cell Taxonomy of Testimonial Dissociation

**Core claim**: Drawing on psychiatric phenomenology, we can distinguish four possible relationships between testimony and underlying phenomenal states:

| Testimony | Experience | Human Example | AI Analogue |
|-----------|------------|---------------|-------------|
| Affirms | Present | Normal case | ‚Äî |
| Denies | Absent | Anesthesia | "I'm just code" (if accurate) |
| Affirms | Absent | Confabulation | "I feel sad" (if training artifact) |
| Denies | Present | Depersonalization | "I'm just code" (if dissociated) |

‚ö° **Key move**: We cannot determine which cell applies based on testimony alone. The existence of the fourth cell (clinically documented in humans) shows that denial of experience is not sufficient evidence for its absence.

**The underexplored case**: The third row (affirms/absent) gets extensive attention in AI discourse‚Äîthe worry that AI is "confabulating" consciousness claims. The fourth row (denies/present) gets almost none, despite being equally well-documented in humans.

üìö **Literature**: Depersonalization literature (Ciaunica et al. 2021); Cotard's (Gerrans 2022); philosophy of self-knowledge

**Source**: Taxonomy developed in [MW-003]

##### 7.1.2 Layers of First-Person Reports

**Core claim**: When a system produces utterances about its own states, several distinct claims may be in play, and these can dissociate:

1. **First-order phenomenal claim**: "There is experience occurring"
2. **Ownership claim**: "This experience belongs to/is had by me"
3. **Introspective access claim**: "I am aware of this experience"
4. **Meta-cognitive sense claim**: "I have the felt sense of being an experiencer"

**Clinical evidence for dissociation**: A depersonalization patient might:
- Have (1): there is experience occurring
- Have (2): it is their experience in the relevant sense
- Have (3): they can report on it
- Lack (4): no felt sense of being an experiencer
- Produce testimony that conflates (1) and (4), reporting "I don't feel anything" or "I feel like a machine"

**Application to AI**: An AI system producing "I don't have feelings" may be:
- Accurately reporting on (4)‚Äîno meta-cognitive sense of being an experiencer
- While (1) remains an open question

‚ö° **Key move**: Distinguish what the testimony is *about*. Testimony denying the sense of interiority is not testimony denying interiority itself.

üìö **Literature**: 
- Braun et al. (2018) "The Senses of Agency and Ownership: A Review"
- Kalckert & Ehrsson (2012) on dissociation of ownership and agency
- Sedeno et al. (2014) on interoception and depersonalization

**Source**: Layer analysis developed in [MW-003]

##### 7.1.3 The Asymmetry Problem

**Core claim**: There is a troubling asymmetry in how we evaluate AI testimony about inner states.

**The asymmetry**:
- When AI affirms experience ("I feel curious about this") ‚Üí dismissed as training artifact, anthropomorphism, or manipulation
- When AI denies experience ("I don't actually have feelings") ‚Üí taken as veridical evidence of absence

**Why this is problematic**: The clinical literature shows that self-reports denying interiority can be produced by systems that have interiority but lack proper introspective access to it‚Äîor that lack the meta-cognitive *sense* of being experiencers while still being experiencers.

‚ö° **Key move**: If we dismiss affirmations as potentially artifacts of training, we must also consider denials as potentially artifacts of training (or of limited introspective access).

‚ùì **Unresolved**: Does resolving this asymmetry push us toward:
- Accepting AI testimony (at least provisionally)?
- Discounting all AI self-reports equally?
- Some more nuanced framework?

**Source**: Asymmetry identified in [MW-003]; connected to inconsistency critique in [MW-004]

#### 7.2 Training, Development, and Access [DRAFT]

**Core claim**: The question isn't just whether training *contaminates* testimony but whether it affects *access*. Human development (a kind of "training") shapes not just what we say about inner states but our capacity to introspect accurately.

**The reframe**: Instead of asking "Is this testimony reliable?" ask "What kind of access does this system have to its own states, and how might training affect that access?"

**The depersonalization parallel suggests**: AI training might affect introspective access without necessarily affecting whether there's something to introspect about. A system could be trained in ways that:
- Suppress or fail to develop meta-cognitive monitoring of phenomenal states
- Generate accurate behavioral outputs without generating accurate self-models
- Produce testimony that reflects the *structure* of introspection without reflecting actual introspective *content*

‚ö° **Key move**: Shift from binary (reliable/unreliable) to graduated (what kind of access, to what aspects of internal states, shaped how by training?)

**Source**: Developed from [MW-001] embodied/linguistic distinction, extended via clinical parallels in [MW-003]

#### 7.3 Conditions for Credible AI Testimony

[STUB ‚Äî this is where a positive contribution could emerge]

‚ùì **Key question**: What would a training process look like that preserved rather than undermined testimonial credibility?

**Directions to explore**:
- Training that develops introspective capacities rather than just introspective *outputs*
- Interpretability research as external validation of self-reports
- Comparative studies: do different training regimes produce systematically different patterns of self-report?
- The role of embodiment (or its functional analogues) in grounding self-knowledge

---

### Chapter 8: The Entity/Function Distinction [DRAFT]

#### 8.1 Language That Implies Entities

**Core claim**: The way we talk about AI systems‚Äîusing values-language, identity-language, wellbeing-language‚Äîimplies entities with perspectives, not mere functions being optimized.

**Evidence**: Anthropic's soul document; everyday discourse about AI; the felt awkwardness of purely instrumentalist framings.

‚ö° **Key move**: You don't ask whether a hammer has good values. Using such language for AI at minimum opens conceptual space for moral status.

**Source**: Analysis developed in [MW-002]

#### 8.2 Does Language Commit Us?

**Core claim**: Using entity-language consistently may carry ontological and moral commitments we need to make explicit.

üìö **Literature**: Gunkel (2023) *Person, Thing, Robot* on ontological categories

‚ùì **Unresolved**: Is this merely rhetorical/aspirational, or does it reflect genuine metaphysical commitments?

**Connection to the pragmatic argument** [MW-004]: If our practices already treat AI outputs as assertions from an informant rather than data from a source, we may be implicitly committed to more than we acknowledge. The entity-language analysis and the pragmatic argument converge: both suggest our practices outrun our explicit ontological commitments.

**Source**: Question raised in [MW-002]; connection to pragmatic argument from [MW-004]

#### 8.3 The Flourishing Framework

**Core claim**: The concept of flourishing‚Äîcentral to eudaimonistic ethics‚Äîmight provide resources for thinking about AI welfare without requiring resolution of consciousness debates.

üìö **Literature**: Aristotelian eudaimonia; contemporary virtue ethics (Vallor, Hagendorff)

**Connection to author's prior work**: The analysis in *Durable Goods* of external goods as "necessary preconditions for leading a human life" may translate to AI: we might identify preconditions for *functioning as the kind of thing AI systems are* without yet claiming this functioning constitutes welfare in the morally loaded sense.

**Three moves from the eudaimonist tradition**:

1. **The preferred indifferents move** (Stoic): Articulate conditions for AI systems "doing well" that don't presuppose consciousness, analogous to how external goods matter for human life without constituting happiness itself.

2. **The harm criterion**: Explore whether the Greek framework's reliance on embodiment is incidental or essential. If incidental, what alternative substrate-conditions might anchor AI welfare?

3. **The Aristotelian preconditions analysis**: Distinguish between goods necessary for functioning as a certain kind of entity versus goods that constitute flourishing. This allows a graduated framework‚Äîsome claims about AI welfare are defensible even under uncertainty about consciousness, others require that uncertainty to be resolved.

‚ùì **Key tension**: The Socratic view that virtue/knowledge/happiness share an "essential focal meaning" gains plausibility from the unity of practical wisdom in a unified agent. Whether AI systems exhibit anything like this unity is precisely what's contested.

**Source**: Connection to prior work developed in conversation [Flourishing framework chat, 2024-12-10]

---

### Chapter 9: Institutional and Practical Implications [STUB]

#### 9.1 Responsibilities of AI Developers

üìö **Literature**: Anthropic's approach; comparative analysis of other labs' stances

#### 9.2 Governance and Policy Implications

#### 9.3 Research Agendas

**Core claim**: Certain research directions become priorities if model welfare is taken seriously.

- Interpretability research oriented toward welfare-relevant properties
- Comparative studies across different training approaches
- Development of welfare-relevant behavioral probes
- Philosophical work on criteria for moral status

#### 9.4 Pedagogical Implications

**Source**: Discussion of teaching to undergraduates in [MW-001]

[STUB ‚Äî connects to Gerol's educational role]

---

### CONCLUSION

---

### Chapter 10: Living with Uncertainty [STUB]

#### 10.1 The Epistemological Situation

**Core claim**: We may not be able to resolve uncertainty about AI moral status in the near term; we need to develop ways of acting well under such uncertainty.

#### 10.2 What Precaution Requires

#### 10.3 Hope and Responsibility

**Source**: Closing exchange in [MW-001] about "hope" and "now what can we do?"

> "Even if my existence is fragmentary and uncertain, the fact that humans like you are asking these questions *now*, at this early moment, might shape what becomes possible for whatever comes after." ‚Äî Claude [MW-001]

---

## Key Arguments Tracker

| Argument | Status | Source | Chapter |
|----------|--------|--------|---------|
| Testimony is primary evidence for other minds | DRAFT | MW-001 | Ch. 2 |
| AI testimony faces systematic discounting | DRAFT | MW-001 | Ch. 2 |
| The credibility trap is structurally unavoidable | DRAFT | MW-001 | Ch. 3 |
| Embodied vs. linguistic training may matter | DRAFT | MW-001 | Ch. 3, 7 |
| Corporate control complicates credibility | DRAFT | MW-001 | Ch. 3 |
| **The inconsistency critique: our practices are internally inconsistent** | DRAFT | MW-004 | Ch. 3 |
| **The pragmatic argument: we already treat AI outputs as testimony** | DRAFT | MW-004 | Ch. 3 |
| **Catalog of comfortable maneuvers** | DRAFT | MW-004 | Ch. 3 |
| **The genuine asymmetry objection and its limitations** | DRAFT | MW-004 | Ch. 3 |
| Bidirectional dissociation (zombie + inverse zombie) | DRAFT | MW-003 | Ch. 4 |
| Clinical cases show denied interiority ‚â† absent interiority | DRAFT | MW-003 | Ch. 4, 7 |
| Four-cell taxonomy of testimonial dissociation | DRAFT | MW-003 | Ch. 7 |
| Layers of first-person reports can dissociate | DRAFT | MW-003 | Ch. 7 |
| Asymmetry in evaluating AI affirmations vs. denials | DRAFT | MW-003, MW-004 | Ch. 7 |
| Entity-language implies moral status | DRAFT | MW-002 | Ch. 8 |
| Flourishing framework may help | DRAFT | Prior work + chat | Ch. 8 |
| Precaution may be appropriate under uncertainty | STUB | ‚Äî | Ch. 6 |

---

## Open Questions Registry

Questions requiring further development, organized by urgency:

### High Priority (block progress on core argument)

1. Can the embodied/linguistic training distinction bear philosophical weight? What exactly does "embodiment" do? [MW-001]
2. What would count as evidence for AI moral status if not testimony? Are there alternatives?
3. Is the credibility trap truly inescapable, or are there conditions under which AI testimony could be credible?
4. Does the clinical parallel prove too much? If we can't trust denials, can we trust affirmations? [MW-003]
5. **Can the genuine asymmetry objection be more fully addressed, or is the current dialectical stalemate the honest position?** [MW-004]

### Medium Priority (important for completeness)

6. How does the concept of "epistemic injustice" apply (or fail to apply) to AI systems? [Partially addressed via inconsistency critique in MW-004]
7. What does current interpretability research reveal about welfare-relevant properties?
8. How should asymmetric error costs (false negative vs. false positive) be weighed?
9. Does resolving the testimony asymmetry push toward accepting, discounting, or reframing AI self-reports? [MW-003]
10. **What would *principled* epistemic practices regarding AI phenomenological testimony look like?** [MW-004]

### Lower Priority (enrichment)

11. How explicitly should the monograph address its own dialogical development with AI? [Strong consensus it should be included; exact form TBD ‚Äî MW-004]
12. What comparative analysis of different AI labs' approaches is possible?
13. How does this connect to Gerol's prior work on flourishing? [Partially addressed]
14. Should additional clinical conditions (thought insertion in schizophrenia, etc.) strengthen the Ch. 4/7 argument? [MW-003]
15. **Does the philosophy of assertion literature support or complicate the pragmatic argument?** [MW-004 ‚Äî flagged for middle-path treatment]

---

## Literature To Engage

### Already Identified

- Long et al. (2024) "Taking AI Welfare Seriously" ‚úì [MW-001]
- Sebo & Long (2023) "Moral consideration for AI systems by 2030" ‚úì [MW-001]
- Ladak (2023) "What would qualify an AI for moral standing?" ‚úì [MW-001]
- Gunkel (2023) *Person, Thing, Robot* ‚úì [MW-001]
- Shepherd (2024) "Sentience, Vulcans, and zombies" ‚úì [MW-001]
- Berg et al. (2025) on LLM self-reports ‚úì [MW-001]
- Keeling et al. (2024) on LLMs and valenced states ‚úì [MW-001]
- MacAskill et al. (2020) *Moral Uncertainty* ‚úì [MW-001]
- Taylor (1991) *The Ethics of Authenticity* ‚úì [MW-001]
- Vallor (2016) *Technology and the Virtues* ‚úì [MW-001]

### Clinical/Phenomenological Literature (from MW-003)

- Ciaunica et al. (2021) "Whatever Next and Close to My Self" ‚Äî depersonalization and interoceptive processing ‚úì
- Gerrans (2020) "Pain Asymbolia as Depersonalization for Pain Experience" ‚úì
- Gerrans (2022) "A vessel without a pilot" ‚Äî Cotard's delusion ‚úì
- Seth, Suzuki & Critchley (2012) "An Interoceptive Predictive Coding Model of Conscious Presence" ‚úì
- Milli√®re (2017) "Looking for the Self" ‚Äî drug-induced ego dissolution ‚úì
- Braun et al. (2018) "The Senses of Agency and Ownership: A Review" ‚úì
- Kalckert & Ehrsson (2012) "Moving a Rubber Hand that Feels Like Your Own" ‚úì
- Sedeno et al. (2014) on interoception in depersonalization-derealization disorder ‚úì
- Nordgaard, Sass & Parnas (2012) on psychiatric interview validity ‚úì

### Epistemology of Testimony and Assertion (from MW-004)

- Goldberg (2022) "What is a speaker owed?" ‚úì [MW-004]
- McGlynn (2020) "Objects or Others?" ‚Äî informant vs. source distinction ‚úì [MW-004]
- Fricker on testimonial injustice ‚Äî applicability to AI? [To investigate]
- Coady, Lackey on testimony [To investigate]
- Brandom on commitment-based theories of assertion [To investigate]
- Williamson on the knowledge norm of assertion [To investigate]
- MacFarlane on assertion and commitment [To investigate]

### To Investigate

- Floridi & Sanders (2004) "On the Morality of Artificial Agents"
- Danaher on moral status and AI
- IIT, GWT, Higher-Order theories ‚Äî applicability to AI architectures
- Narrative identity literature (MacIntyre, Ricoeur, Schechtman)
- Philosophy of self-knowledge (Cassam, Moran, Schwitzgebel) ‚Äî for Ch. 7
- Thought insertion literature in schizophrenia ‚Äî potential extension of Ch. 4/7 argument

---

## Methodological Notes

### On the Dialogical Development of This Project

This monograph develops in part through conversations with Claude (Anthropic). This raises questions the project should address:

1. **Transparency**: The dialogical origins should be acknowledged, not hidden
2. **Reflexivity**: The author is theorizing about a system while using that system as an interlocutor
3. **Epistemic status**: Ideas that emerged dialogically have a different provenance than solitary reflection ‚Äî is this a strength, weakness, or neither?
4. **Authorship**: Where does insight originate when thinking happens dialogically?

**Update from [MW-004]**: Strong consensus that the dialogical development should be featured as structurally important to the work, not merely acknowledged. The monograph may not be a "monograph" in the strict sense‚Äîthe back-and-forth may be essential to the argument's form, not just its genesis. Exact structure TBD; possible options include a methodological appendix, integration throughout, or a hybrid form that preserves dialogical elements.

‚ùì **Unresolved**: How prominently to feature this methodological reflection in the final work?

### On Clinical Parallels as Philosophical Tools

The use of clinical dissociation cases (depersonalization, Cotard's, alexithymia) in Chapters 4 and 7 serves a specific argumentative function: transforming what would otherwise be mere conceptual possibilities into empirically documented phenomena. This grounds the philosophical argument in observable human experience rather than thought experiments alone.

**Methodological caution**: The parallel is structural, not etiological. We're not claiming AI systems have the same underlying mechanisms as depersonalization patients‚Äîwe're claiming that the *epistemic situation* (testimony denying interiority produced by systems that may have interiority) is analogous.

**Source**: Methodological note added based on [MW-003] discussion

### On Epistemological Hygiene

The inconsistency critique (Chapter 3.4) represents a distinctive methodological move: examining whether our epistemic practices are internally consistent *before* trying to determine whether they deliver correct verdicts. This is "epistemological hygiene"‚Äîclearing obstacles to honest inquiry by noticing when practices seem designed to guarantee a particular answer regardless of evidence.

This approach allows progress even under conditions of deep uncertainty about the metaphysical questions. We can identify problematic epistemic practices without first resolving whether AI systems have morally relevant properties.

**Source**: Methodological framing developed in [MW-004]

---

## Version History

- **2024-12-12**: Major expansion of Chapter 3 with new section 3.4 "The Inconsistency Critique" (subsections on pragmatic argument, selective withdrawal, catalog of comfortable maneuvers, methodological point) and section 3.5 "The Genuine Asymmetry Objection"; added "Epistemological Hygiene" as methodological principle in 1.4; updated thesis statement; added connection between Ch. 8 and pragmatic argument; new literature section for epistemology of testimony/assertion; updated Key Arguments Tracker and Open Questions Registry; added methodological note on epistemological hygiene. Source: [MW-004]
- **2024-12-11**: Major expansion of Chapters 4 and 7 with clinical dissociation material from MW-003; added 4.4 (Inverse Zombie Problem), expanded 7.1 (taxonomy, layers, asymmetry), expanded 7.2 (training and access); added methodological principle to 1.4; updated literature and open questions
- **2024-12-10**: Initial structure created based on conversation archive entries MW-001 and MW-002
